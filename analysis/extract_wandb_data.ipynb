{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import os\n",
    "\n",
    "project_name = \"CANDID_DAC\"\n",
    "\n",
    "# download all runs from a project\n",
    "api = wandb.Api()\n",
    "# there is a little bug in the API that might return duplicate runs but omit others, that is why we\n",
    "# have to use order='-heartbeat_at' (you might need play around with that as different settings seem to work for different people )\n",
    "runs = api.runs(project_name, order='-heartbeat_at')\n",
    "\n",
    "distinct_run_ids = set()\n",
    "duplicate_run_ids = set()\n",
    "\n",
    "print(f\"Number of runs (total): {len(runs)}\")\n",
    "\n",
    "for run in runs:\n",
    "    if run.id in distinct_run_ids:\n",
    "        duplicate_run_ids.add(run.id)\n",
    "    else:\n",
    "        distinct_run_ids.add(run.id)\n",
    "\n",
    "# due to the aforementioned bug, we also check that there are no duplicate run ids\n",
    "print(f\"Number of distinct run ids: {len(distinct_run_ids)}\")\n",
    "print(f\"Number of duplicate run ids: {len(duplicate_run_ids)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# create a directory \"run_updated_metrics\" if it does not exist\n",
    "if not os.path.exists(\"run_data\"):\n",
    "    os.makedirs(\"run_data\")\n",
    "\n",
    "# define paths to store the metrics and the run configuration\n",
    "metrics_path = f'run_data/{project_name}_metrics.csv'\n",
    "config_path = f'run_data/{project_name}_configs.csv'\n",
    "\n",
    "# load the project data from a csv file if it already exists\n",
    "if os.path.exists(metrics_path):\n",
    "    existing_metrics = pd.read_csv(metrics_path)\n",
    "    existing_configs = pd.read_csv(config_path)\n",
    "    print(f'Found {len(existing_configs)} existing runs')\n",
    "else:\n",
    "    existing_metrics = pd.DataFrame()\n",
    "    existing_configs = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_run_ids = set([])\n",
    "\n",
    "for run in runs:\n",
    "    if run.Group == \"reproduce\":\n",
    "        continue\n",
    "    project_run_ids.add(run.id)\n",
    "print(f'Found {len(project_run_ids)} unique runs in the project to download')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# remove all runs that from the existing metrics that are not in the project run ids\n",
    "if len(existing_metrics) > 0:\n",
    "    existing_metrics = existing_metrics[existing_metrics['run_id'].isin(project_run_ids)]\n",
    "if len(existing_configs) > 0:\n",
    "    existing_configs = existing_configs[existing_configs['run_id'].isin(project_run_ids)]\n",
    "\n",
    "existing_metric_ids = set(existing_metrics['run_id'].unique())\n",
    "existing_config_ids = set(existing_configs['run_id'].unique())\n",
    "\n",
    "run_ids_to_reload = set([])\n",
    "# also remove metrics of runs to reload\n",
    "if len(existing_metrics) > 0:\n",
    "    existing_metrics = existing_metrics[~existing_metrics['run_id'].isin(run_ids_to_reload)]\n",
    "reload_all = False\n",
    "\n",
    "# if reload_all, add all run ids to the list of run_ids_to_reload\n",
    "if reload_all:\n",
    "    run_ids_to_reload = existing_metric_ids\n",
    "\n",
    "run_ids_to_download = project_run_ids - (existing_metric_ids - run_ids_to_reload)\n",
    "already_downloaded = len(existing_metric_ids) - len(run_ids_to_reload)\n",
    "\n",
    "\n",
    "to_download = len(run_ids_to_download)\n",
    "\n",
    "print(f\"Already downloaded: {already_downloaded}, to download: {to_download} (reloads: {len(run_ids_to_reload)})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**get all runs from a wandb project and extract steps and avg episod reward into a dataframe, mark all data with the run id**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "downloading = 1\n",
    "\n",
    "new_metrics = []\n",
    "new_config_dict = []\n",
    "\n",
    "# remove all metrics that should be reloaded\n",
    "if len(run_ids_to_reload) > 0:\n",
    "    existing_metrics = existing_metrics[~existing_metrics['run_id'].isin(run_ids_to_reload)]\n",
    "    existing_configs = existing_configs[~existing_configs['run_id'].isin(run_ids_to_reload)]\n",
    "\n",
    "not_finished = 0\n",
    "for run in runs:\n",
    "    if run.Group == \"reproduce\" or run.id not in run_ids_to_download:\n",
    "        continue\n",
    "    run_metrics = []\n",
    "    print(f\"downloading run {downloading}/{to_download} ({run.name})\", end=\"\\r\")\n",
    "\n",
    "    if not run.state == \"finished\":\n",
    "        not_finished += 1\n",
    "        downloading += 1\n",
    "        continue\n",
    "\n",
    "    # get the metrics from the run\n",
    "    run_metrics.append(run.history(keys=[\"_step\", \"avg_episodic_reward\"], samples=100000))\n",
    "    if \"avg_reward_train_set\" in run.summary_metrics:\n",
    "        run_metrics.append(run.history(keys=[\"_step\", \"avg_reward_train_set\"], samples=100000))\n",
    "    if \"avg_reward_test_set\" in run.summary_metrics:\n",
    "        run_metrics.append(run.history(keys=[\"_step\", \"avg_reward_test_set\"], samples=100000))\n",
    "    # merge the dataframes on the step column, using an outer join\n",
    "    data_run = pd.merge(run_metrics[0], run_metrics[1], on=\"_step\", how=\"outer\").merge(run_metrics[2], on=\"_step\", how=\"outer\")\n",
    "    data_run[\"run_id\"] = run.id\n",
    "\n",
    "    new_metrics.append(pd.DataFrame(data_run))\n",
    "    \n",
    "    # get the config from the run\n",
    "    config = run.config\n",
    "    config[\"run_id\"] = run.id\n",
    "    config[\"run_name\"] = run.name\n",
    "    new_config_dict.append(config)\n",
    "    \n",
    "    downloading += 1\n",
    "print(f\"Downloaded {downloading-1} runs, {not_finished} runs were not finished.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**some checks before saving the data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_metrics = pd.concat([existing_metrics] + new_metrics, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Unique run ids: {len(updated_metrics['run_id'].unique())}\")\n",
    "steps_per_id = updated_metrics.groupby('run_id').agg({'_step': 'max'})\n",
    "deviating_ids = steps_per_id[steps_per_id['_step'] != 100000.0]\n",
    "print(f\"Number of ids with deviating number of steps: {len(deviating_ids)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# single data points might get lost during download but make sure the final performance is available\n",
    "steps_per_id[steps_per_id['_step'] != 100000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_configs = pd.concat([existing_configs, pd.DataFrame(new_config_dict)], ignore_index=True)\n",
    "print(f\"Unique run ids in the config: {len(updated_configs['run_id'].unique())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**If everything is fine store the data.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the avg episodic reward data to a csv file\n",
    "import time\n",
    "\n",
    "updated_metrics.to_csv(metrics_path, index=False)\n",
    "print(f\"Stored {len(updated_metrics)} metric values to {metrics_path} at {time.strftime('%H:%M:%S')}\")\n",
    "# store the config data to a csv file\n",
    "updated_configs.to_csv(config_path, index=False)\n",
    "print(f\"Stored {len(updated_configs)} runs to {config_path} at {time.strftime('%H:%M:%S')}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

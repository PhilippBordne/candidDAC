{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define what we want to plot**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings(category=UserWarning, action=\"ignore\", module='dacbench')\n",
    "warnings.filterwarnings(category=FutureWarning, action=\"ignore\", module='dacbench')\n",
    "\n",
    "project_name = 'CANDID_DAC'\n",
    "config_id = 'best'\n",
    "metric_to_plot = \"avg_reward_test_set\"\n",
    "\n",
    "# benchmarks_to_compare = ['sigmoid', 'candid_sigmoid', 'piecewise_linear']\n",
    "benchmarks_to_compare = ['sigmoid', 'piecewise_linear']\n",
    "dim = 5\n",
    "# for piecewise_linear or candid_sigmoid we will have to specify action importances and parameters of reward function\n",
    "reward_shape = 'exponential'\n",
    "exp_reward= 4.6\n",
    "importance_base = 0.5\n",
    "n_act = 3\n",
    "\n",
    "path_to_config = f'../run_data/{project_name}_configs.csv'\n",
    "\n",
    "df_config = pd.read_csv(path_to_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Get the runs per benchmark we want to plot**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotting_helpers import get_run_configs\n",
    "\n",
    "runs_per_benchmark = {}\n",
    "\n",
    "for benchmark in benchmarks_to_compare:\n",
    "    run_configs_on_bench = get_run_configs(df_config, dim=dim, benchmark=benchmark, config_id=config_id,\n",
    "                                           importance_base=importance_base, reward_shape=reward_shape, exp_reward=exp_reward,\n",
    "                                           n_act=n_act)\n",
    "    seeds_per_algorithm = run_configs_on_bench.groupby('algorithm')['seed'].aggregate(['count', list])\n",
    "    seeds_per_algorithm['list'] = seeds_per_algorithm['list'].apply(lambda x: sorted(x))\n",
    "\n",
    "    # remove pandas max column width, and also increase the width of the table\n",
    "    pd.set_option('display.max_colwidth', None)\n",
    "    pd.set_option('display.width', 1000)\n",
    "    print(f'For benchmark {benchmark} we have the following runs:')\n",
    "    print(seeds_per_algorithm)\n",
    "    runs_per_benchmark[benchmark] = run_configs_on_bench.groupby('algorithm')['run_id'].aggregate(list)\n",
    "    run_configs_on_bench.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from plotting_helpers import METHOD_COLOURS, get_run_configs, translate_run_name, get_best_possible_avg_reward\n",
    "\n",
    "metrics_path = f'../run_data/{project_name}_metrics.csv'\n",
    "df_metrics = pd.read_csv(metrics_path)\n",
    "\n",
    "plot_sig_on_train = True  # if True and plotting is usually done on test set, additionally train set performances on sigmoid\n",
    "\n",
    "# width = 4\n",
    "# fig, axes = plt.subplots(1, len(benchmarks_to_compare), figsize=(len(benchmarks_to_compare) * width, width), sharey=True, sharex=True)\n",
    "plt.rcParams.update({\n",
    "    'font.size': 8,           # Global font size\n",
    "    'axes.titlesize': 7,      # Title size of individual plots\n",
    "    'axes.labelsize': 7,      # Label size for x and y labels\n",
    "    'xtick.labelsize': 6.5,      # Size of x-tick labels\n",
    "    'ytick.labelsize': 6.5,      # Size of y-tick labels\n",
    "    'legend.fontsize': 7,      # Size of the legend text\n",
    "    'figure.titlesize': 12,     # Title size of the entire figure\n",
    "    'lines.linewidth': 0.75\n",
    "})\n",
    "height = 1.2\n",
    "width = 3.5*height\n",
    "dpi=100  # set to 100 to get in size of paper not zoomed in\n",
    "fig, axes = plt.subplots(1, len(benchmarks_to_compare), figsize=(width, height), sharey=True, dpi=dpi, )\n",
    "\n",
    "for i, benchmark in enumerate(benchmarks_to_compare):\n",
    "    ax = axes[i]\n",
    "    configs = runs_per_benchmark[benchmark]\n",
    "\n",
    "    if benchmark in ['candid_sigmoid', 'piecewise_linear']:\n",
    "        optim_1D = get_best_possible_avg_reward(5, benchmark=benchmark, reward_shape=reward_shape, c=exp_reward, importance_base=importance_base, max_dim=1)\n",
    "        optim_5D = get_best_possible_avg_reward(5, benchmark=benchmark, reward_shape=reward_shape, c=exp_reward, importance_base=importance_base, max_dim=5)\n",
    "\n",
    "        ax.plot([0, 10000], [optim_5D, optim_5D], color='black', linestyle='--', label='optimal')\n",
    "        ax.plot([0, 10000], [optim_1D, optim_1D], color='dimgray', linestyle='--', label='optimal (1D)')\n",
    "        if benchmark == 'piecewise_linear':\n",
    "            ax.set_title(f'{dim}D Piecewise Linear', y=0.94)\n",
    "        else:\n",
    "            ax.set_title(f'{dim}D CANDID Sigmoid', y=0.94)\n",
    "    else:\n",
    "        optim_5D = get_best_possible_avg_reward(5, benchmark=benchmark, reward_shape=reward_shape, c=exp_reward, importance_base=importance_base, max_dim=5)\n",
    "        ax.plot([0, 10000], [optim_5D, optim_5D], color='black', linestyle='--', label='Accumulated optimal')\n",
    "        ax.set_title(f'{dim}D Sigmoid', y=0.94)\n",
    "\n",
    "    for run_name, run_ids in configs.items():\n",
    "        df_rewards = df_metrics[df_metrics['run_id'].isin(run_ids)]\n",
    "\n",
    "        run_name = run_name.rsplit('_', 1)[0]\n",
    "        label = translate_run_name(run_name)\n",
    "\n",
    "        # remove all rows containing NaN values for the metric to plot\n",
    "        if benchmark == 'sigmoid' and plot_sig_on_train:\n",
    "            df_train = df_metrics[df_metrics['run_id'].isin(run_ids)]\n",
    "            df_train = df_train.dropna(subset=['avg_reward_train_set'])\n",
    "            df_train = df_train.groupby('_step')['avg_reward_train_set'].agg('mean')\n",
    "            ax.plot(df_train.index / 10, df_train, linestyle=':', color=METHOD_COLOURS[run_name])\n",
    "\n",
    "        df_rewards = df_rewards.dropna(subset=[metric_to_plot]) \n",
    "        df_rewards = df_rewards.groupby('_step')[metric_to_plot].agg(['mean', 'std'])\n",
    "\n",
    "\n",
    "        ax.set_ylim(0, 10)\n",
    "        ax.set_xlim(0, 10000)\n",
    "        ax.set_yticks(np.linspace(0, 10, 5))\n",
    "        ax.set_xticks(np.linspace(0, 10000, 5))\n",
    "        ax.tick_params(axis='x', pad=-5)\n",
    "        ax.set_xlabel('Episodes')\n",
    "\n",
    "        # plot mean and std\n",
    "        ax.plot(df_rewards.index / 10, df_rewards['mean'], label=label, color=METHOD_COLOURS[run_name])\n",
    "        ax.fill_between(df_rewards.index / 10, df_rewards['mean'] - df_rewards['std'], df_rewards['mean'] + df_rewards['std'],\n",
    "                        color=METHOD_COLOURS[run_name], alpha=0.2)\n",
    "        \n",
    "\n",
    "y_label = 'Avg. episodic reward'\n",
    "axes[0].set_ylabel(y_label)\n",
    "\n",
    "leg = axes[-1].legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "\n",
    "if plot_sig_on_train:\n",
    "    from matplotlib.lines import Line2D\n",
    "    handles, labels = axes[-1].get_legend_handles_labels()\n",
    "    handles += [Line2D([0], [0], linestyle=':', color='gray')]\n",
    "    labels += ['on train set']\n",
    "    axes[-1].legend(handles=handles, labels=labels, loc='center left', bbox_to_anchor=(1, 0.6))\n",
    "fig.subplots_adjust(wspace=0.15)\n",
    "fig.savefig(f'paper_plots/sig_vs_pl.png', bbox_inches='tight', dpi=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "marl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

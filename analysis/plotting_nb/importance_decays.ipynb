{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(category=UserWarning, action=\"ignore\", module=\"dacbench\")\n",
    "warnings.filterwarnings(category=FutureWarning, action=\"ignore\", module=\"dacbench\")\n",
    "import pandas as pd\n",
    "\n",
    "project_name = \"CANDID_DAC\"\n",
    "metrics_path = f\"../run_data/{project_name}_metrics.csv\"\n",
    "configs_path = f\"../run_data/{project_name}_configs.csv\"\n",
    "benchmark = \"piecewise_linear\"\n",
    "\n",
    "config_id = \"best\"\n",
    "\n",
    "# set up the experiment we want to plot\n",
    "reward_shape = 'exponential'\n",
    "exp_reward = 4.6\n",
    "importance_bases = [0.3, 0.5, 0.7]\n",
    "dim = 5\n",
    "n_act = 3\n",
    "metric_to_plot = \"avg_reward_test_set\"\n",
    "\n",
    "df_configs = pd.read_csv(configs_path)\n",
    "df_metrics = pd.read_csv(metrics_path)\n",
    "\n",
    "# already filter the data frames to only contain the data for the relevant config_ids\n",
    "if config_id == \"best\":\n",
    "    # only keep rows where config_id contains \"best\"\n",
    "    df_configs = df_configs[df_configs[\"config_id\"].str.contains(\"best\")]\n",
    "else:\n",
    "    # otherwise the config_id is the one we want to plot\n",
    "    df_configs = df_configs[df_configs[\"config_id\"] == config_id]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Filter the config dataframe to only contain experiments that are of interest to us**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter the data frames to only contain the data for the relevant config_ids\n",
    "if config_id == \"best\":\n",
    "    # only keep rows where config_id contains \"best\"\n",
    "    df_configs = df_configs[df_configs[\"config_id\"].str.contains(\"best\")]\n",
    "else:\n",
    "    # otherwise the config_id is the one we want to plot\n",
    "    df_configs = df_configs[df_configs[\"config_id\"] == config_id]\n",
    "\n",
    "# print(df_configs.head())\n",
    "\n",
    "# get the configs per benchmark dimensionality\n",
    "configs = df_configs[(df_configs['benchmark'] == benchmark) &\n",
    "                     (df_configs['reward_shape'] == reward_shape) &\n",
    "                     (df_configs['exp_reward'] == exp_reward) &\n",
    "                     (df_configs['importance_base'].isin(importance_bases)) &\n",
    "                     (df_configs['dim'] == dim) & \n",
    "                     (df_configs['reverse_agents'] == False) &\n",
    "                     (df_configs['n_act'] == n_act)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_configs = configs.groupby(['run_name', 'importance_base']).agg(list)['run_id']\n",
    "# aggregate the seeds in the groups as list to control that all seeds are available for every setup\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "seeds_in_group = configs.groupby(['run_name', 'importance_base']).agg(list)['seed']\n",
    "# order the seeds in seeds_in_group\n",
    "seeds_in_group = seeds_in_group.apply(lambda x: sorted(x))\n",
    "print(seeds_in_group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from plotting_helpers import get_best_possible_avg_reward, translate_run_name, METHOD_COLOURS\n",
    "\n",
    "plt.rcParams.update({\n",
    "    'font.size': 8,           # Global font size\n",
    "    'axes.titlesize': 7,      # Title size of individual plots\n",
    "    'axes.labelsize': 7,      # Label size for x and y labels\n",
    "    'xtick.labelsize': 6.5,      # Size of x-tick labels\n",
    "    'ytick.labelsize': 6.5,      # Size of y-tick labels\n",
    "    'legend.fontsize': 7,      # Size of the legend text\n",
    "    'figure.titlesize': 12,     # Title size of the entire figure\n",
    "    'lines.linewidth': 0.75\n",
    "})\n",
    "height = 1.2\n",
    "width = 5*height\n",
    "fig, axes = plt.subplots(1, len(importance_bases), figsize=(width, height), sharey=True)\n",
    "\n",
    "for i, importance_base in enumerate(importance_bases):\n",
    "    ax = axes[i]\n",
    "\n",
    "    # plot the best possible reward\n",
    "    optimal_reward_1D = get_best_possible_avg_reward(dim, benchmark=benchmark,\n",
    "                                                     reward_shape=reward_shape, c=exp_reward, importance_base=importance_base, max_dim=1)\n",
    "    optimal_reward_acc = get_best_possible_avg_reward(dim, benchmark=benchmark,\n",
    "                                                     reward_shape=reward_shape, c=exp_reward, importance_base=importance_base, max_dim=dim)\n",
    "\n",
    "    print(f'Optimal reward for {dim}D: {optimal_reward_acc} (acc), {optimal_reward_1D} (1D)')\n",
    "\n",
    "    ax.plot([0, 10000], [optimal_reward_acc, optimal_reward_acc], label='optimal', color='black', linestyle='--')\n",
    "    ax.plot([0, 10000], [optimal_reward_1D, optimal_reward_1D], label='optimal (1D)', color='grey', linestyle='--')\n",
    "\n",
    "    # only keep the runs for this dimensionality\n",
    "    importance_configs = grouped_configs.xs(importance_base, level='importance_base')\n",
    "\n",
    "    # now plot rewards per approach, which is encoded by run_name\n",
    "    for run_name, run_ids in importance_configs.items():\n",
    "        run_name = run_name.rsplit('_', 1)[0]\n",
    "        label = translate_run_name(run_name)\n",
    "        color = METHOD_COLOURS[run_name]\n",
    "\n",
    "        # get the rewards for this run\n",
    "        # remove all rows in the metrics, where the metric to plot is NaN\n",
    "        rewards = df_metrics[df_metrics[metric_to_plot].notna()]\n",
    "        rewards = rewards.loc[df_metrics['run_id'].isin(run_ids)].groupby('_step')[metric_to_plot].agg(['mean', 'std'])\n",
    "        \n",
    "        # plot the mean and std\n",
    "        ax.fill_between(rewards.index / 10, rewards['mean'] - rewards['std'], rewards['mean'] + rewards['std'], alpha=0.5, color=color)\n",
    "        ax.plot(rewards.index / 10, rewards['mean'], label=label, color=color)\n",
    "\n",
    "        # make the plot square, regardless of scaling difference in x and y\n",
    "        ax.set_ylim(2, 10.05)\n",
    "        ax.set_xlim(0, 10000)\n",
    "        ax.set_xticks(np.linspace(0, 10000, 5))\n",
    "        ax.set_yticks(np.linspace(0, 10, 5))\n",
    "\n",
    "        ax.set_title(f'Importance decay $\\lambda$={importance_base}')\n",
    "        ax.set_xlabel('Episodes')\n",
    "\n",
    "# set the same y-axis label for all plots\n",
    "y_label = 'Avg. episodic reward'\n",
    "# if metric_to_plot == 'avg_reward_test_set':\n",
    "#     y_label += ' (test set)'\n",
    "# elif metric_to_plot == 'avg_reward_train_set':\n",
    "#     y_label += ' (train set)'\n",
    "# elif metric_to_plot == 'avg_episodic_reward':\n",
    "#     y_label += ' (training)'\n",
    "axes[0].set_ylabel(y_label)\n",
    "# set a shared legend, right to the rightmost plot\n",
    "axes[-1].legend(loc='center left', bbox_to_anchor=(1, 0.5), ncol=1)\n",
    "\n",
    "# reduce space between plots and title\n",
    "plt.subplots_adjust(wspace=0.15)\n",
    "title = f'{dim}D {\"Piecewise Linear\" if benchmark == \"piecewise_linear\" else \"CANDID Sigmoid\"}'\n",
    "# fig.suptitle(title, fontweight='bold')\n",
    "fig.savefig(f'./paper_plots/pl_importance_decays.png', dpi=600, bbox_inches='tight')\n",
    "# plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "marl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
